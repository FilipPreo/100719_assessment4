{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Module 4 Assessment</h1>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This assessment is designed to test your understanding of the Mod 4 material. It covers:\n",
    "\n",
    "* Calculus, Cost Function, and Gradient Descent\n",
    "* Introduction to Logistic Regression\n",
    "* Extensions to Linear Models\n",
    "\n",
    "Read the instructions carefully. You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "### Note on the short answer questions\n",
    "\n",
    "For the short answer questions please use your own words. The expectation is that you have not copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, do your best to communicate yourself clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculus, Cost Function, and Gradient Descent [Suggested Time: 25 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![best fit line](visuals/best_fit_line.png)\n",
    "\n",
    "The best fit line that goes through the scatterplot up above can be generalized in the following equation: $$y = mx + b$$\n",
    "\n",
    "Of all the possible lines, we can prove why that particular line was chosen using the plot down below:\n",
    "\n",
    "![](visuals/cost_curve.png)\n",
    "\n",
    "where RSS is defined as the residual sum of squares:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is a more generalized name for the RSS curve above? How is it related to machine learning models?\n",
    "#### Hint: RSS is just one of many metrics that could be used to do the same job. We are asking about that general use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The RSS curve is an example of a cost function curve. We use the cost function curve to find the minimum value of the cost so we can optimize our model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Would you rather choose a $m$ value of 0.08 or 0.05 from the RSS curve up above?   What is the relation between the position on the cost curve, the error, and the slope of the line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    "I would rather choose 0.05. The slope of the line is used to predict a set of values (our y_predicted). The error (loss function) is then calculated from these values and the actual values (y_test). Then the cost function is calculated from the error. Different values of m will give us different values of the cost function and we want to choose the value of m that gives us the lowest possible value of the cost function. In this case 0.05 is closer to this desired value than 0.08. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](visuals/gd.png)\n",
    "\n",
    "### 3. Using the gradient descent visual from above, explain why the distance between each step is getting smaller as more steps occur with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "Gradient descent calculates the slope / gradient of the curve and changes the value of m in order to reduce the cost function. Effectively, gradient descent makes our m-value change by a step in a certain direction. The size of that step is proportional to the negative of the steepness of the cost curve at the point where it is evaluated. At point 1 the curve is steepest. So the gradient will be a larger, positive value. So the step will be a larger, negative number. As the model produces a new value of RSS at 2, the gradient is calculated again and is found to be a bit less steep, therefore the step along the m-axis will be smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the purpose of a learning rate in gradient descent? Explain how a very small and a very large learning rate would affect the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "In gradient descent, the change in our model's weights (i.e. the step we take along the axis of m-values) is proportional to the learning rate X (- the slope of the cost function curve). \n",
    "\n",
    "So for every training epoch, the model changes the value of m by an amount proportional to the learning rate (LR) we set. The learning rate is a hyperparameter - it can be set arbitrarily by us. If we want our model to train very quickly and we can set a higher value of LR; moreoever, if we think the model might converge on a local and not on a global minimum (this can be pictured as a graph with two valleys - one valley deeper than the other), then we setting a high LR might help the model to avoid this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction to Logistic Regression [Suggested Time: 25 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# load data\n",
    "ads_df = pd.read_csv(\"raw_data/social_network_ads.csv\")\n",
    "\n",
    "# one hot encode categorical feature\n",
    "def is_female(x):\n",
    "    \"\"\"Returns 1 if Female; else 0\"\"\"\n",
    "    if x == \"Female\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "ads_df[\"Female\"] = ads_df[\"Gender\"].apply(is_female)\n",
    "ads_df.drop([\"User ID\", \"Gender\"], axis=1, inplace=True)\n",
    "ads_df.head()\n",
    "\n",
    "# separate features and target\n",
    "X = ads_df.drop(\"Purchased\", axis=1)\n",
    "y = ads_df[\"Purchased\"]\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n",
    "\n",
    "# preprocessing\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# save preprocessed train/test split objects\n",
    "pickle.dump(X_train, open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(X_test, open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"write_data/social_network_ads/y_train.pkl\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"write_data/social_network_ads/y_test.pkl\", \"wb\"))\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create confusion matrix\n",
    "# tn, fp, fn, tp\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "cnf_matrix\n",
    "\n",
    "# build confusion matrix plot\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "# Add title and Axis Labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Add appropriate Axis Scales\n",
    "class_names = set(y_test) #Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()\n",
    "\n",
    "# Add padding\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visuals/cnf_matrix.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cnf matrix](visuals/cnf_matrix.png)\n",
    "\n",
    "### 1. Using the confusion matrix up above, calculate precision, recall, and F-1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our recall is  0.9310344827586207\n",
      "Our precision is  0.7142857142857143\n",
      "Our F1 is  0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# // your code here //\n",
    "# precision / sensitivity : TPR - (TP / (positive condition))\n",
    "# recall  :  TNR - (TN / (negative condition))\n",
    "# F-1 score: TP / TN\n",
    "\n",
    "prec = 30 / (30+12)\n",
    "recall = 54 / (54 + 4)\n",
    "F1 = 30 / 54\n",
    "\n",
    "print(\"Our recall is \", recall)\n",
    "print(\"Our precision is \", prec)\n",
    "print(\"Our F1 is \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  What is a real life example of when you would care more about recall than precision? Make sure to include information about errors in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    "Since recall is the rate of false negatives, this would be a case where the consequence / cost of getting a false negative would be greater than the cost of getting a false positive. For instance, if you are diagnosing patients and deciding whether they have a certain disease (positive) or not (negative) you would care less about identifying people who don't aren' sick as being sick, and more about identifying people who are sick as not being sick. \n",
    "The consequence for a false positive is smaller (a healthy person gets treatment, asuming the treatment is not extreme). \n",
    "The consequence for a false negative is greater (a sick person doesn't get treatment and might get more sick and / or die). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# save preprocessed train/test split objects\n",
    "X_train = pickle.load(open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"rb\"))\n",
    "X_test = pickle.load(open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"rb\"))\n",
    "y_train = pickle.load(open(\"write_data/social_network_ads/y_train.pkl\", \"rb\"))\n",
    "y_test = pickle.load(open(\"write_data/social_network_ads/y_test.pkl\", \"rb\"))\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "labels = [\"Age\", \"Estimated Salary\", \"Female\", \"All Features\"]\n",
    "colors = sns.color_palette(\"Set2\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "# add one ROC curve per feature\n",
    "for feature in range(3):\n",
    "    # female feature is one hot encoded so it produces an ROC point rather than a curve\n",
    "    # for this reason, female will not be included in the plot at all since it is\n",
    "    # disingeneuous to call it a curve.\n",
    "    if feature == 2:\n",
    "        pass\n",
    "    else:\n",
    "        X_train_feat = X_train[:, feature].reshape(-1, 1)\n",
    "        X_test_feat = X_test[:, feature].reshape(-1, 1)\n",
    "        logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='lbfgs')\n",
    "        model_log = logreg.fit(X_train_feat, y_train)\n",
    "        y_score = model_log.decision_function(X_test_feat)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color=colors[feature],\n",
    "                 lw=lw, label=labels[feature])\n",
    "\n",
    "# add one ROC curve with all the features\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "y_score = model_log.decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color=colors[3], lw=lw, label=labels[3])\n",
    "\n",
    "# create foundation of the plot\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i / 20.0 for i in range(21)])\n",
    "plt.xticks([i / 20.0 for i in range(21)])\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visuals/many_roc.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pick the best ROC curve from this graph and explain your choice. \n",
    "\n",
    "*Note: each ROC curve represents one model, each labeled with the feature(s) inside each model*.\n",
    "\n",
    "<img src = \"visuals/many_roc.png\" width = \"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The \"all features\" curve is the best ROC curve. The measure of an ROC's curve effectiveness is given by the total area under the curve (AUC). Since the \"all features\" curve shoots up towards the top left corner of the plot first and gives us the best ratio of true positive rate to false positive rate, we can safely say that it is the best model, since it has the greatest AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# sorting by 'Purchased' and then dropping the last 130 records\n",
    "dropped_df = ads_df.sort_values(by=\"Purchased\")[:-130]\n",
    "dropped_df.reset_index(inplace=True)\n",
    "pickle.dump(dropped_df, open(\"write_data/sample_network_data.pkl\", \"wb\"))\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original classifier has an accuracy score of 0.956.\n",
      "The original classifier has an area under the ROC curve of 0.836.\n"
     ]
    }
   ],
   "source": [
    "network_df = pickle.load(open(\"write_data/sample_network_data.pkl\", \"rb\"))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop(\"Purchased\", axis=1)\n",
    "y = network_df[\"Purchased\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train,y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f\"The original classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.\")\n",
    "\n",
    "# get the area under the curve from an ROC curve\n",
    "y_score = model.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc = round(roc_auc_score(y_test, y_score), 3)\n",
    "print(f\"The original classifier has an area under the ROC curve of {auc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The model above has an accuracy score that might be too good to believe. Using `y.value_counts()`, explain how `y` is affecting the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    257\n",
       "1     13\n",
       "Name: Purchased, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# // your answer here //\n",
    "\n",
    "The value counts shows that we have a highly imbalanced data set (there are 13 true positive cases and 257 true negative cases). The model is can very easily predict the true negatives and hence has such a high accuracy score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What methods would you use to address the issues mentioned up above in question 4? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# // your answer here //\n",
    "I'd illustrate the results using a confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extensions to Linear Regression [Suggested Time: 25 min]\n",
    "---\n",
    "\n",
    "In this section, you're going to be creating linear models that are more complicated than a simple linear regression. In the cells below, we are importing relevant modules that you might need later on. We also load and prepare the dataset for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the optimal number of degrees for our polynomial features in this model? In general, how does increasing the polynomial degree relate to the Bias/Variance tradeoff?  (Note that this graph shows RMSE and not MSE.)\n",
    "\n",
    "<img src =\"visuals/rsme_poly_2.png\" width = \"600\">\n",
    "\n",
    "<!---\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "degree = list(range(1, 10 + 1))\n",
    "ax.plot(degree, error_train[0:len(degree)], \"-\", label=\"Train Error\")\n",
    "ax.plot(degree, error_test[0:len(degree)], \"-\", label=\"Test Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Polynomial Feature Degree\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Relationship Between Degree and Error\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"visuals/rsme_poly.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "The optimal degree of our polynomial model is 3, because the error for the training and testing set are both at a similarly low rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In general what methods could you use to reduce overfitting and underfitting? Provide an example for both and explain how each technique works to reduce the problems of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "We would use KFold cross-validation to prevent both problems. This would split our **training** data set into K sections, train and model on (K-1) of those sections and test it on the *Kth* section. This is repeated K times, each time a different section being the test-sample within the training dataset. This will give us a list of R squared values and a list of (R)MSEs for each iteration. \n",
    "If we see a large variation in our R2 values, then we are overfitting and need to alter our model, either by reducing the polynomial degree or removing features. We would take the mean R2 and (R)MSE values as the values for our training data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the difference between the two types of regularization for linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "Both of the two regularization models add a penalty term to our cost function that is proportional to the size of the coefficient. In LASSO regression it is proportional to the size of the coefficient. In Ridge regression it is proportional to the square of the coefficient. \n",
    "\n",
    "The result of this is that in Ridge, larger coefficient are penalised very heavily and reduced in size, however, as they get smaller (below 1.0), the penalty of Ridge on the coefficient size becomes smaller and smaller. This means that Ridge regression does not make coefficients reach zero, they just get asymptotically closer.\n",
    "Because Lasso penalizes coefficients based directly on their size, coefficients in a Lasso model can eventually reach zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why is scaling input variables a necessary step before regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "Because different features in a model can have wildly varying ranges of absolute values (e.g. the age of a car might range between 0 and 30, but the car weight in kg can be between 500 and 1500), a model trained on unscaled data would have to compensate for the features with the smaller absolute values and produce highly varying coefficients. If we scale the data, all the values of each feature end up being between -1 and 1, so the model does not have to compensate for differences in absolute value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------\n",
    "                                           OPTIONAL\n",
    "### -------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as snsc\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('raw_data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing set. Do not change the random state please!\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We'd like to add a bit of complexity to the model created in the example above, and we will do it by adding some polynomial terms. Write a function to calculate train and test error for different polynomial degrees.\n",
    "\n",
    "This function should:\n",
    "* take `degree` as a parameter that will be used to create polynomial features to be used in a linear regression model\n",
    "* create a PolynomialFeatures object for each degree and fit a linear regression model using the transformed data\n",
    "* calculate the mean square error for each level of polynomial\n",
    "* return the `train_error` and `test_error` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(degree):\n",
    "    \"\"\"\n",
    "    Calculate train and test errorfor a linear regression with polynomial features.\n",
    "    (Hint: use PolynomialFeatures)\n",
    "    \n",
    "    input: Polynomial degree\n",
    "    output: Mean squared error for train and test set\n",
    "    \"\"\"\n",
    "    # // your code here //\n",
    "    \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    poly_train_x = poly.fit_transform(X_train)\n",
    "    poly_test_x = poly.transform(X_test)\n",
    "    \n",
    "    linreg = LinearRegression()\n",
    "    model = linreg.fit(poly_train_x, y_train)\n",
    "    y_train_hat = linreg.predict(poly_train_x)\n",
    "    y_test_hat = linreg.predict(poly_test_x)    \n",
    "    \n",
    "    train_error = mean_squared_error(y_train, y_train_hat)\n",
    "    test_error = mean_squared_error(y_test, y_test_hat)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24235967358392063, 0.15281375976869446)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18179109287485934, 1.952259624453726)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your answers\n",
    "\n",
    "MSE for degree 3:\n",
    "- Train: 0.2423596735839209\n",
    "- Test: 0.15281375973923944\n",
    "\n",
    "MSE for degree 4:\n",
    "- Train: 0.18179109317368244\n",
    "- Test: 1.9522597174462015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
